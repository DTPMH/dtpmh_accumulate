{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Machine Learning for Systems & Control 5SC27 2020-2021\n", "\n", "# Exercise set W2 Gaussian Processes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this exercise set, you will implement your own version of a Radial Basis Function Gaussian Process, explore properties and explore how it can be used. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 1: Writing your own GP\n", "\n", "**a)** Based on Slide 32 and 20 for Lecture 2, implement GP regression with $\\sigma_e^2 = 0.1$ (expected noise variance) and a squared exponential kernel with $\\sigma^2 = 0.1$ (kernel width) using the `lownoise.mat` (place contents of the `dataW2.zip` in the same folder as the notebook) data set and plot the resulting function estimate on a fine grid of `xtest = np.linspace(-1,1,num=300)`. Analyse the qualitative fit w.r.t. the data. \n", "\n", "*tip: start by (i) finishing the `kernal` function than use the `kernal` function in (i) `compute_alpha` and use that in (iii) `pred_mean_and_var` to compute the mean.*\n", "\n", "*tip: use np.linalg.solve instead of np.linalgh.inv*\n", "\n", "$\\hat \\alpha = \\left ( K_{xx} + \\sigma_e^2 I_N \\right ) ^{-1} Y$\n", "\n", "**b)** Compute the resulting variance of the function estimate (slide 20) of 1.a and plot 2 time the standard deviation (95% confidence bound) of the function. You can use the second part of `pred_mean_and_var` to complete this."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.io import loadmat\n", "import numpy as np\n", "\n", "######### selecting dataset 'low', 'med' or 'high'  #########\n", "ident = 'low'\n", "\n", "#load data\n", "out = loadmat(f'{ident}noise.mat') #train data\n", "x = out['x'][:,0] #shape=(N,)\n", "y = out['y'][:,0] #shape=(N,)\n", "out = loadmat(f'valid_{ident}noise.mat') #validation data\n", "xval = out['x'][:,0] #shape=(Nval,)\n", "yval = out['y'][:,0] #shape=(Nval,)\n", "\n", "xtest = np.linspace(-1,1,num=300) #xpoints for visualization\n", "\n", "\n", "from matplotlib import pyplot as plt\n", "plt.plot(x,y,'.')\n", "plt.show()\n", "\n", "def kernal(x1, x2, sigma2_ker):\n", "    #make a matrix for two given inputs arrays\n", "    #input: \n", "    #x1 of shape (N1)\n", "    #x2 of shape (N2)\n", "    #sigma2_ker float the squared kernel width\n", "    \n", "    #output\n", "    #Kxx of shape (N1,N2)\n", "    # a) Fill this\n", "    return Kxx\n", "\n", "def compute_alpha(x, y, sigma2_es, sigma2_ker):\n", "    #for a given x and y data computes the alpha and Kxx \n", "    #uses the kernal function from above\n", "\n", "    Kxx = # a) Fill this\n", "    alpha = # a) Fill this\n", "    return alpha, Kxx #return both alpha and Kxx (required for variance estimation)\n", "\n", "    \n", "    \n", "def pred_mean_and_var(xtest, x, Kxx, alpha, sigma2_es, sigma2_ker, return_std=True): \n", "    #conditionally return the standard deviation  \n", "    K = lambda x1,x2: kernal(x1, x2, sigma2_ker=sigma2_ker) #shorthand for the kernal function use as K(x1,x2)\n", "    \n", "    Ktx = # a) Fill this\n", "    Ypred_mean = # a) Fill this\n", "    \n", "    if not return_std:\n", "        return Ypred_mean\n", "\n", "    #finished at b)\n", "#     Ypred_var = \n", "    # b) Fill this\n", "    return Ypred_mean, Ypred_var**0.5\n", "    \n", "sigma2_es = 0.1\n", "sigma2_ker = 0.1\n", "alpha, Kxx = compute_alpha(x, y, sigma2_es, sigma2_ker) # a) finish compute_alpha  \n", "Ypred_mean = pred_mean_and_var(xtest, x, Kxx, alpha, sigma2_es, sigma2_ker, return_std=False) # a) finish pred_mean_and_var  \n", "\n", "plt.plot(x,y,'.')\n", "plt.plot(xtest,Ypred_mean)\n", "plt.ylabel('y'); plt.xlabel('x'); plt.legend(['training data','mean predicted value'])\n", "plt.show()\n", "\n", "if False: #switch to true when on b)\n", "    Ypred_mean, Ypred_std = pred_mean_and_var(xtest, x, Kxx, alpha, sigma2_es, sigma2_ker)\n", "    plt.plot(x,y,'.')\n", "    plt.plot(xtest,Ypred_mean,'.')\n", "    plt.fill_between(xtest,Ypred_mean-2*Ypred_std,Ypred_mean+2*Ypred_std,alpha=0.3)\n", "    plt.ylabel('y'); plt.xlabel('x'); plt.legend(['training data','mean predicted value','predicted std'])\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**c)** Write down the influence of the kernel width $\\sigma$ and the $\\sigma_e$ on the predictions using the figures generated in the cell below.\n", "\n", "**Answer c):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["def plot(x,y,xtest,sigma2_es,sigma2_ker):\n", "    \n", "    alpha, Kxx = compute_alpha(x, y, sigma2_es, sigma2_ker)\n", "    Ypred_mean, Ypred_std = pred_mean_and_var(xtest, x, Kxx, alpha, sigma2_es, sigma2_ker)\n", "    \n", "    plt.title(f'$\\\\sigma^2 = {sigma2_ker}$, $\\\\sigma^2_e = {sigma2_es}$ ')\n", "    plt.plot(x,y,'.')\n", "    plt.plot(xtest,Ypred_mean)\n", "    plt.fill_between(xtest,Ypred_mean-2*Ypred_std,Ypred_mean+2*Ypred_std,alpha=0.3)\n", "    plt.tight_layout()\n", "\n", "\n", "plt.figure(figsize=(12,8))\n", "i = 0\n", "for sigma2_es in [0.01,0.1,1.0]:\n", "    for sigma2_ker in [0.01,0.1,1.0]:\n", "        i+=1\n", "        plt.subplot(3,3,i)\n", "        plot(x,y,xtest,sigma2_es,sigma2_ker)\n", "\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**d)** Implement a grid search for $\\sigma_e$ and $\\sigma$ using the validation set (`xval` and `yval`). Complete the RMS function below and iterate over both arrays to create the grid. Use the cell after the next one to visualize the result. Does this result seem sensible?\n", "\n", "**Answer d):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def RMS(x, y, xval, yval, sigma2_es, sigma2_ker):\n", "    # d) Fill this\n", "sigma2_es_list = np.geomspace(0.001,3,num=21) #increasing value in log space\n", "sigma2_ker_list = np.geomspace(0.001,2,num=22) #increasing value in log space\n", "\n", "\n", "# d) Fill this\n", "RMS_mat = # d) Fill this\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#plotting for d)\n", "plt.contour(sigma2_ker_list, sigma2_es_list, np.clip(RMS_mat,-float('inf'),np.percentile(RMS_mat.flat,80)))\n", "plt.loglog()\n", "plt.colorbar()\n", "plt.xlabel(r'$\\sigma^2$')\n", "plt.ylabel(r'$\\sigma_e^2$')\n", "plt.show()\n", "\n", "best1, best2 = np.unravel_index(np.argmin(RMS_mat),RMS_mat.shape)\n", "sigma2_ker_list[best2], sigma2_es_list[best1]\n", "plot(x,y,xtest,sigma2_es_list[best1],sigma2_ker_list[best2])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**e)** Do the same for the marginal log-likelihood and afterwards use the `minimize` from `scipy.optimize` to find the maximum.\n", "\n", "$$\n", "\\text{loglike} = 1/N \\sum_i \\log \\left ( \\frac{1}{\\sigma \\sqrt{2 \\pi} }  e^{-(y_i-\\hat{y_i})^2/(2 \\sigma_y^2)}  \\right ) \n", "$$\n", "\n", "*tip: rewrite the expression above such to reduce floating-point errors*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loglike(x, y, xval, yval, sigma2_es, sigma2_ker):\n", "    alpha, Kxx = compute_alpha(x, y, sigma2_es, sigma2_ker)\n", "    yval_mean_pred, yval_std = pred_mean_and_var(xval, x, Kxx, alpha, sigma2_es, sigma2_ker, return_std=True) \n", "    \n", "    \n", "    loglike = # e) Fill this\n", "\n", "    return loglike\n", "\n", "sigma2_es_list = np.geomspace(0.001,2,num=21)\n", "sigma2_ker_list = np.geomspace(0.001,2,num=23)\n", "mat_out_like = []\n", "for sigma2_es in sigma2_es_list:\n", "    print(sigma2_es)\n", "    mat_out_row = []\n", "    for sigma2_ker in sigma2_ker_list:\n", "        mat_out_row.append(loglike(x, y, xval, yval, sigma2_es, sigma2_ker))\n", "    mat_out_like.append(mat_out_row)\n", "mat_out_like = np.array(mat_out_like)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#plotting for e)\n", "plt.contour(sigma2_ker_list, sigma2_es_list, np.clip(mat_out_like,np.percentile(mat_out_like.flat,30),float('inf')))\n", "plt.colorbar()\n", "plt.loglog()\n", "plt.show()\n", "best1, best2 = np.unravel_index(np.argmax(mat_out_like),mat_out_like.shape)\n", "print(sigma2_ker_list[best2], sigma2_es_list[best1])\n", "print(np.max(mat_out_like))\n", "plot(x,y,xtest,sigma2_es_list[best1],sigma2_ker_list[best2])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.optimize import minimize\n", "#fun(x, *args) -> float (see docs)\n", "\n", "# maximize marginal log likelihood\n", "# e) Fill this\n", "fsol = # e) Fill this\n", "# fsol = minimize(...)\n", "\n", "sigma2_es_best, sigma2_ker_best = fsol.x\n", "plot(x,y,xtest,sigma2_es_best, sigma2_ker_best)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 2: Using sklearn GP\n", "\n", "Now that you understand the basics of gaussian processing let's switch to a nicely implemented version included in sklearn [1.7 Gaussian Processes](https://scikit-learn.org/stable/modules/gaussian_process.html). It includes features like:\n", "\n", "* The hyperparameters of the kernel are optimized during the fitting of `GaussianProcessRegressor` by maximizing the log-marginal-likelihood (LML) (and using `scipy.optimize.minimize`). \n", "* Different kernels can be specified. Common kernels are provided, but it is also possible to specify custom kernels.\n", "\n", "**a)** Noise term $\\sigma_e$ can be interpreted as a kernel and is used as such in sklearn. Which kind of kernel $k(x_i, x_j)$ would produce the desired behaviour of a noise term.\n", "\n", "**Answer a):** fill by student\n", "\n", "**b)** Using the x and y data generated below construct a kernel as a Radial Basis Function `RBF` and `WhiteKenel` using `+` and estimate the model. Show the resulting model with `.predict` (set `return_std=True`).\n", "\n", "*tip: read the documentation of gaussian processes provided here [function doc](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html), [User guide](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process)*\n", "\n", "*tip: set n_restarts_optimizer=10 for more robust hyperparamter optimization*\n", "\n", "*note: sklearn is multi-variate inputs so the x arrays need to have the shape of `(Nsamp, Nfeatures)` with `Nfeatures=1`*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# f0 = lambda x: np.sin(3*x) + 0.5*np.random.normal(loc=0,scale=0.9,size=x.shape)\n", "import numpy as np\n", "from matplotlib import pyplot as plt\n", "np.random.seed(43)\n", "N = 100\n", "noise = 0.1\n", "f0 = lambda x: np.sin(3*x)\n", "\n", "x = np.random.normal(loc=0,scale=0.8,size=N)\n", "y = f0(x) + noise*np.random.normal(loc=0,scale=0.9,size=x.shape)\n", "xtest = np.linspace(-4,4,num=150)\n", "ytest = f0(xtest)\n", "\n", "plt.plot(x,y,'.')\n", "plt.show()\n", "\n", "\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared\n", "\n", "\n", "#construct kernal\n", "ker = # b) Fill this\n", "#construct regressor\n", "reg = # b) Fill this\n", "#fit regressor\n", "reg.fit(x[:,None],y) \n", "#use regressor\n", "ytest_p, ytest_std = # b) Fill this\n", "\n", "#plot result\n", "plt.plot(xtest,ytest,'k')\n", "plt.plot(xtest,ytest_p)\n", "plt.xlim(min(xtest),max(xtest))\n", "plt.fill_between(xtest,ytest_p-2*ytest_std,ytest_p+2*ytest_std,alpha=0.3)\n", "plt.plot(x,y,'.')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "Gaussian processes are probabilistic in nature, hence we can sample them. \n", "\n", "**c)** Sample the obtained Gaussian process using `reg.sample_y` 7 times on the test set and plot and interpret the results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ysamps = # c) Fill this\n", "\n", "plt.plot(xtest,ysamps,alpha=0.7)\n", "plt.fill_between(xtest,ytest_p-2*ytest_std,ytest_p+2*ytest_std,alpha=0.3)\n", "plt.xlim(min(xtest),max(xtest))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**d)** Repeat the exercise trying out some other kernels provided in sklearn [Kernals](https://scikit-learn.org/stable/modules/gaussian_process.html#kernel-operators), for instance, the Exp-Sine-Squared kernel (`ExpSineSquared`)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["As seen in the implementation solving a GP requires solving a system of equations of $A x = y$ with a $A$ a shape of `(Nsamp,Nsamp)` which scales worst than linear with increasing data size. \n", "\n", "**e)** Measure the time it takes to estimate a model for different dataset sizes for `range(100,2500,200)` and save the time it takes to an array and observe the scaling by plotting these saved times. (this question should not take more than a minute)\n", "\n", "*tip: use time.time() to get the current time in seconds from the time module*\n", "\n", "**f)** How does the computation time scale with the number of samples (linear or worse?)\n", "\n", "**Answer f):** fill by student\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared\n", "import numpy as np\n", "def test(Nsamples):\n", "    N = Nsamples\n", "    noise = 0.1\n", "    f0 = lambda x: np.sin(3*x)\n", "\n", "    x = np.random.normal(loc=0,scale=0.8,size=N)\n", "    y = f0(x) + noise*np.random.normal(loc=0,scale=0.9,size=x.shape)\n", "\n", "    reg = GaussianProcessRegressor(RBF(length_scale=1) + WhiteKernel(noise_level=1.0)) \n", "    reg.fit(x[:,None],y) \n", "    return reg\n", "\n", "import time\n", "Nsamples_list = range(100,2500,200)\n", "# e) Fill this\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#plotting here\n", "# e) Fill this\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 3: NARX\n", "\n", "From the last exercise, we saw that GP can estimate non-linear models. For this exercise we will apply it to the same example we have in the last exercise set (Week 1). \n", "\n", "**a)** Construct the data arrays `Xtrain, Xval, Ytrain, Yval` using the cell below and estimate a GP with RBF and white kernel. Just as before making a residual plot of both the training and validation data. Now also include the uncertainty in the residual plot as a bar plot ([matplotlib errorbar plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot as plt\n", "def f(upast,ypast):\n", "    ukm2, ukm1 = upast\n", "    ykm2, ykm1 = ypast\n", "    ystar = (0.8 - 0.5 * np.exp(-ykm1 ** 2)) * ykm1 - (0.3 + 0.9 * np.exp(-ykm1 ** 2)) * ykm2 \\\n", "           + ukm1 + 0.2 * ukm2 + 0.1 * ukm1 * ukm2\n", "    return ystar + np.random.normal(scale=0.01)\n", "\n", "def get_NARX_data(ulist, f, na, nb):\n", "    #init upast and ypast as lists.\n", "    upast = [0]*nb \n", "    ypast = [0]*na \n", "    \n", "    ylist = []\n", "    for unow in ulist:\n", "        #compute the current y given by f\n", "        ynow = f(upast,ypast) \n", "        \n", "        #update past arrays\n", "        upast.append(unow)\n", "        upast.pop(0)\n", "        ypast.append(ynow)\n", "        ypast.pop(0)\n", "        \n", "        #save result\n", "        ylist.append(ynow)\n", "    return np.array(ylist) #return result\n", "\n", "na, nb = 2, 2\n", "\n", "np.random.seed(42)\n", "N = 500\n", "ulist = np.random.normal(scale=1,size=N)\n", "ylist = get_NARX_data(ulist,f,na,nb)\n", "\n", "def make_training_data(ulist,ylist,na,nb):\n", "    #Xdata = (Nsamples,Nfeatures)\n", "    #Ydata = (Nsamples)\n", "    Xdata = []\n", "    Ydata = []\n", "    #for loop over the data:\n", "    for k in range(max(na,nb),len(ulist)): #skip the first few indexes such to \n", "        Xdata.append(np.concatenate([ulist[k-nb:k],ylist[k-na:k]])) \n", "        Ydata.append(ylist[k]) \n", "    return np.array(Xdata), np.array(Ydata)\n", "\n", "Xdata, Ydata = make_training_data(ulist,ylist, 2, 2)\n", "\n", "from sklearn.model_selection import train_test_split \n", "Xtrain, Xval, Ytrain, Yval = train_test_split(Xdata, Ydata) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#fitting\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared\n", "\n", "ker = # e) Fill this\n", "reg = # e) Fill this\n", "# e) Fill this\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#residual calculations and plotting\n", "# e) Fill this\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**b)** Just as before we are also interested in the simulator performance of the model. Make a simulation and plot the residual and the NRMS. Is it lower than the polynomial model of the last exercise set?\n", "\n", "**c)** Retry the exercise with different kernels and see if you can construct a kernel that is more accurate."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "np.random.seed(43)\n", "utest = np.random.normal(scale=1.0,size=5000)\n", "ytest = get_NARX_data(utest,f,na,nb)\n", "\n", "\n", "model_now = # b) Fill this\n", "ytest_sim = get_NARX_data(utest,lambda u,y: model_now.predict(np.concatenate([u,y])[None,:])[0], na, nb)\n", "# b) Fill this\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise 4: Bayesian optimization\n", "\n", "In this exercise, we will explore the basics of Bayesian Optimization. Consider the same setting as in Exercise 2, but now with a slightly modified process as seen below. \n", "\n", "**a)** In this case, we will only use 10 randomly generated training samples (N = 10) using the measurement function `f` and we will iteratively add new samples to refine our prediction for the possible function values. Define the test set as `xtest = np.linspace(-3,3,num=1000)`. After training for the 10 initial samples compute the standard deviation of the predicted function values over the test set and chose the location where it is the largest. Now, add the chosen test point to the training set and compute its corresponding y by the given formula `f`. Repeat this process until you have 20 samples and see how the resulting function estimate and its 95% confidence bound improving. Do you get a better estimate than with 20 randomly chosen samples? Plot the results below\n", "\n", "*tip: use np.argmax and np.append*\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared\n", "warnings.filterwarnings(\"ignore\") #there might be some warning but this will supress them.\n", "\n", "f0 = lambda x: np.sin(3*x) + 0.2*x + 0.3 #no noise version\n", "f = lambda x: f0(x) + np.random.normal(scale=0.002,size=np.array(x).shape) #noisy version\n", "\n", "Ntotal = 20 #number of total points\n", "Nstart = 10 #inital points\n", "\n", "x = # a) Fill this\n", "y = # a) Fill this\n", "\n", "xtest = # a) Fill this\n", "\n", "def get_model(x,y): \n", "    #return a regressor which is fitted to x,y\n", "    # a) Fill this\n", "for i in range(Ntotal-Nstart):\n", "    # a) Fill this\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["reg_op = get_model(x,y)\n", "xrand = np.random.uniform(low=-3,high=3,size=Ntotal)\n", "yrand = f(xrand)\n", "reg_rand = get_model(xrand,yrand)\n", "\n", "for xnow, ynow, reg in [(x,y,reg_op), (xrand,yrand,reg_rand)]:\n", "    ytest_pred_mean, ytest_pred_std = reg.predict(xtest[:,None],return_std=True)\n", "    plt.plot(xtest,f0(xtest),'k')\n", "    plt.plot(xtest,ytest_pred_mean)\n", "    plt.plot(xnow,ynow,'.r')\n", "    plt.fill_between(xtest,ytest_pred_mean-2*ytest_pred_std,ytest_pred_mean+2*ytest_pred_std,alpha=0.4)\n", "    plt.show()\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**b)** We can also use the same methodology to explore the maximum value of the function. Apply the same approach as above, but, in this case, select the new training point as the maximum of the weighted sum of the mean and the standard deviation, like: \n", "\n", "`((1-weight)*ytest_pred_mean+weight*ytest_pred_std)` Use first weight=0.5. Do you get the maximum of the function after 10 iterations? Explore the effect of choosing the weight to be high (\u00e2\u2030\u00a5 0.85) and low (\u00e2\u2030\u00a4 0.25)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "f0 = lambda x: np.sin(3*x) + 0.2*x + 0.3 #no noise version\n", "f = lambda x: f0(x) + np.random.normal(scale=0.002,size=np.array(x).shape) #noisy version\n", "\n", "Ntotal = 20\n", "Nstart = 10\n", "weight = # b) Fill this\n", "\n", "x = np.random.uniform(low=-3,high=3,size=Nstart)\n", "y = f(x)\n", "\n", "xtest = np.linspace(-3,3,num=1000)\n", "\n", "def get_model(x,y):\n", "    ker = RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01) \n", "    reg = GaussianProcessRegressor(ker, n_restarts_optimizer=10) \n", "    reg.fit(x[:,None],y)\n", "    return reg\n", "\n", "\n", "for i in range(Ntotal-Nstart):\n", "    reg = get_model(x,y) \n", "    ytest_pred_mean, ytest_pred_std = reg.predict(xtest[:,None],return_std=True) \n", "    xnew = # b) Fill this\n", "    ynew = f(xnew) \n", "    x = np.append(x,xnew) \n", "    y = np.append(y,ynew) \n", "plt.plot(x,y,'.') \n", "plt.show() \n", "\n", "reg_op = get_model(x,y)\n", "xrand = np.random.uniform(low=-3,high=3,size=Ntotal)\n", "yrand = f(xrand)\n", "reg_rand = get_model(xrand,yrand)\n", "\n", "for xnow, ynow, reg in [(x,y,reg_op), (xrand,yrand,reg_rand)]:\n", "    ytest_pred_mean, ytest_pred_std = reg.predict(xtest[:,None],return_std=True)\n", "    plt.plot(xtest,f0(xtest),'k')\n", "    plt.plot(xtest,ytest_pred_mean)\n", "    plt.plot(xnow,ynow,'.r')\n", "    plt.fill_between(xtest,ytest_pred_mean-2*ytest_pred_std,ytest_pred_mean+2*ytest_pred_std,alpha=0.4)\n", "    plt.show()\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The grid-based optimization that we used above is not guaranteed to find precisely global\n", "optimum of the selection problem, neither it is applicable in case of high dimensions of x.\n", "Hence, direct maximization over the covariance function is suggested,\n", "however, in that case the problem of finding the global optimum can still seriously affect the\n", "outcome. Hence random initializations or swarm optimization can achieve better outcomes.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}