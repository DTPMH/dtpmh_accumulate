{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-af03c8b6c301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m  \u001b[1;31m## Import the OpenAI gym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m  \u001b[1;31m## Import the Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m  \u001b[1;31m## Import Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m  \u001b[1;31m## Import random for probability generation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m  \u001b[1;31m## For plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import gym  ## Import the OpenAI gym\n",
    "import keras  ## Import the Keras\n",
    "import numpy as np  ## Import Numpy\n",
    "import random  ## Import random for probability generation\n",
    "import matplotlib.pyplot as plt  ## For plotting\n",
    "from random import shuffle  ## For shuffling the Replay Memory\n",
    "\n",
    "from gym import wrappers  ## gym wrappers for monitoring\n",
    "from keras.models import Sequential  ## Sequential model from Keras\n",
    "from keras.layers import Dense  ## Dense Layer from Keras\n",
    "from keras.optimizers import Adam  ## Adam Optimizer from Keras\n",
    "from keras.models import load_model  ## For saving and loading the Keras model\n",
    "\n",
    "from collections import deque  ## Deque for storing samples in Replay Memory \n",
    "\n",
    "ACTIONS_DIM = 2  ## Output Dimension=No. of possible actions (2)\n",
    "OBSERVATIONS_DIM = 4  ## Input Dimension= No of Elements in State Tuple (4)\n",
    "MAX_ITERATIONS = 500  ## Max Time Steps Per Game (Limited to 500 by Environment)\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "GAMMA = 0.99  ## Future Reward Discount Factor\n",
    "REPLAY_MEMORY_SIZE = 10000  ## Replay Memory Size\n",
    "NUM_EPISODES = 600  ## Games Played in Training Phase\n",
    "MINIBATCH_SIZE = 64  ## Number of Samples chosen randomly from Replay Memory\n",
    "\n",
    "RANDOM_ACTION_DECAY = 0.99  ## The factor by which Random Action Probability Decreases\n",
    "INITIAL_RANDOM_ACTION = 1  ## Initial Random Action Factor\n",
    "Samples=[]  ## A list to store Individual Game Scores\n",
    "Means=[]  ## A list to store Mean Score over Last 20 Games\n",
    "\n",
    "\n",
    "####\n",
    "##  @class ReplayBuffer()\n",
    "##  This class is used to initialize, store and update the Replay Memory\n",
    "####\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):  ## Initialize the Replay Memory\n",
    "        self.max_size = max_size  ## Set Maximum size to REPLAY_MEMORY_SIZE\n",
    "        self.transitions = deque()  ## Initialize a deque to store all samples\n",
    "\n",
    "    def add(self, observation, action, reward, observation2):  ## Function to add sample to the memory\n",
    "        if len(self.transitions) > self.max_size:  ## If size exceeds the max limit, remove an item\n",
    "            if np.random.random()<0.5:\n",
    "                shuffle(self.transitions)  ## Shuffle the Replay Memory\n",
    "            self.transitions.popleft()  ## Remove a Sample\n",
    "        self.transitions.append((observation, action, reward, observation2))  ## Add a Sample to memory\n",
    "\n",
    "    def sample(self, count):  ## Function to randomly select samples=Minibatch size(count)\n",
    "        return random.sample(self.transitions, count)  ## Select from transitions(Memory)\n",
    "\n",
    "    def size(self):  ## Function to keep track of Replay Memory Size \n",
    "        return len(self.transitions)  ## Returns the length of the Replay Memory\n",
    "\n",
    "def get_q(model, observation):  ## Function to Predict Q-Values from the Model\n",
    "    np_obs = np.reshape(observation, [-1, OBSERVATIONS_DIM])  ## Reshape the state\n",
    "    return model.predict(np_obs)  ## Query the Model for possible actions and corresponding Q-Values\n",
    "\n",
    "def train(model, observations, targets):  ## Function to Train the Model\n",
    "    np_obs = np.reshape(observations, [-1, OBSERVATIONS_DIM])  ## Reshape the State\n",
    "    np_targets = np.reshape(targets, [-1, ACTIONS_DIM])  ## Reshape the Target\n",
    "\n",
    "    model.fit(np_obs, np_targets, epochs=1, verbose=0)  ## Fit the model using State-Target Pairs\n",
    "\n",
    "def predict(model, observation):  ## Function to Predict Q-Values from Model\n",
    "    np_obs = np.reshape(observation, [-1, OBSERVATIONS_DIM])  ## Reshape the State\n",
    "    return model.predict(np_obs)  ## Query the Model for possible actions and corresponding Q-Values\n",
    "\n",
    "def get_model():  ## Build the Deep Q-Network\n",
    "    model = Sequential()  ## Type of Model\n",
    "    ####\n",
    "    ## Input layer of Dimension 4 and a Hidden Layer of 24 nodes. Activation 'relu'\n",
    "    ####\n",
    "    model.add(Dense(24, input_shape=(OBSERVATIONS_DIM, ), activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))  ## Add second Hidden Layer of 24 nodes. Activation 'relu'\n",
    "    model.add(Dense(2, activation='linear'))  ## Add output layer of dimension 2. Activation 'linear'\n",
    "\n",
    "    model.compile(  ## Compile the Model\n",
    "        optimizer=Adam(lr=LEARNING_RATE),  ## Adam Optimizer with Initial Learning Rate=0.001 \n",
    "        loss='mse',  ## MSE Loss\n",
    "        metrics=[],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def update_action(action_model, target_model, sample_transitions):  ## Update the model\n",
    "    random.shuffle(sample_transitions)  ## Randomly Shuffle the Minibatch Samples\n",
    "    batch_observations = []  ## Initialize State(Observation) List\n",
    "    batch_targets = []  ## Initialize Target(Output Label) List\n",
    "\n",
    "    for sample_transition in sample_transitions:  ## For each sample in Minibatch\n",
    "        ## Separate each part of observation\n",
    "        old_observation, action, reward, observation = sample_transition\n",
    "        \n",
    "        ####\n",
    "        ## Reshape targets to output dimension(=2)\n",
    "        ####\n",
    "        targets = np.reshape(get_q(action_model, old_observation), ACTIONS_DIM)\n",
    "        targets[action] = reward  ## Set Target Value\n",
    "        if observation is not None:  ## If observation is not Empty\n",
    "            \n",
    "            ####\n",
    "            ## Query the Model for possible actions and corresponding Q-Values\n",
    "            ####\n",
    "            predictions = predict(target_model, observation) \n",
    "            new_action = np.argmax(predictions)  ## Select the Best Action (Max Q-Value)\n",
    "            ## Update the Target with Future Reward Discount Factor\n",
    "            targets[action] += GAMMA * predictions[0, new_action]\n",
    "            \n",
    "        batch_observations.append(old_observation)  ## Add Old State to observations batch\n",
    "        batch_targets.append(targets)  ## Add target to targets batch\n",
    "\n",
    "    ## Update the model using Observations and their corresponding Targets\n",
    "    train(action_model, batch_observations, batch_targets)\n",
    "\n",
    "def main():\n",
    "    Temp=[]  ## Initialize a list to hold most recent 100 Game Scores\n",
    "    iteration=0  ## Initialize Time Step Number to Avoid initial No variable Error\n",
    "    ## Set Random Action Probability to Initial Number(=1)\n",
    "    random_action_probability = INITIAL_RANDOM_ACTION\n",
    "    \n",
    "    replay = ReplayBuffer(REPLAY_MEMORY_SIZE)  ## Initialize Replay Memory & Specify Maximum Capacity\n",
    "\n",
    "    action_model = get_model()  ## Initialize action-value model with random weights\n",
    "\n",
    "    env = gym.make('CartPole-v1')  ## Prepare the OpenAI Cartpole-v1 Environment\n",
    "\n",
    "    for episode in range(NUM_EPISODES):  ## For Games 0 to Maximum Games Limit\n",
    "        ## If mean over the last 100 Games is >495, then Success!!!\n",
    "        if np.mean(Temp)>495 and iteration>495:\n",
    "            print('Passed')  ## Print the information that the model is converged\n",
    "            break  ## Terminate after convergence\n",
    "            \n",
    "        ## Reduce the Random Action Probability by Decay Factor\n",
    "        random_action_probability *= RANDOM_ACTION_DECAY\n",
    "        observation = env.reset()  ## Reset the Environment after Each Game\n",
    "\n",
    "        for iteration in range(MAX_ITERATIONS):  ## Timesteps\n",
    "            ## Generate Random Action Probability\n",
    "            random_action_probability = max(random_action_probability, 0.1)\n",
    "            old_observation = observation  ## Store Current State\n",
    "            ## If generated fraction<Random Action Probability\n",
    "            if np.random.random() < random_action_probability:\n",
    "                ## Take Random Action (Explore)\n",
    "                action = np.random.choice(range(ACTIONS_DIM))\n",
    "            else:  ## If generated fraction>Random Action Probability\n",
    "                ## Query the Model and Get Q-Values for possible actions\n",
    "                q_values = get_q(action_model, observation)\n",
    "                action = np.argmax(q_values)  ## Select the Best Action using Q-Values received\n",
    "            ## Take the Selected Action and Observe Next State\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            if done:  ## If Game Over\n",
    "                Samples.append(iteration+1)  ## Add Final Score of the Game to the Scores List\n",
    "                ## If Number of Games>100, Calculate Mean Over 100 Games to Check Convergence\n",
    "                if len(Samples)>100:\n",
    "                    Temp=Samples[-100:]  ## Select Score of Most Recent 100 Games\n",
    "                    ## Add Mean of Most Recent 20 Games to a list\n",
    "                    Means.append(np.mean(Samples[-20:]))\n",
    "                ## Print End-Of-Game Information\n",
    "                print(('Episode:{}, iterations:{}, RAP:{}').format(\n",
    "                        episode,\n",
    "                        iteration,random_action_probability))\n",
    "\n",
    "                if iteration!=499:\n",
    "                    reward = -5  ## Give -5 Reward for Taking Wrong Action Leading to Failure\n",
    "                if iteration==499:\n",
    "                    reward= 5  ## Give +5 Reward for Completing the Game Successfully\n",
    "                ## Add the Observation to Replay Memory\n",
    "                replay.add(old_observation, action, reward, None)\n",
    "                break  ## Break and Start a new Game\n",
    "                \n",
    "            ## Add the Observation to Replay Memory\n",
    "            replay.add(old_observation, action, reward, observation)\n",
    "\n",
    "            ####\n",
    "            ##  Update the Deep Q-Network Model\n",
    "            ####\n",
    "            if replay.size() >= MINIBATCH_SIZE and np.random.random()<0.25 and Samples[-1]<495:\n",
    "                sample_transitions = replay.sample(MINIBATCH_SIZE)\n",
    "                update_action(action_model, action_model, sample_transitions)\n",
    "\n",
    "\n",
    "####\n",
    "##  Here, the Training Phase Ends. Now We plot the Training Results and Save the Trained Model\n",
    "##  We Also Test the Model for Another 100 Games.\n",
    "####\n",
    "\n",
    "    print(\"Training Done\")\n",
    "    plt.plot(Samples)\n",
    "    plt.title('Training Phase')\n",
    "    plt.ylabel('Time Steps')\n",
    "    plt.ylim(ymax=510)\n",
    "    plt.xlabel('Trial')\n",
    "    plt.savefig('Training.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(Means)\n",
    "    plt.title('Mean Score of Last 20 Games')\n",
    "    plt.ylabel('Time Steps')\n",
    "    plt.ylim(ymax=510)\n",
    "    plt.xlabel('Trial')\n",
    "    plt.savefig('Training_Average.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    action_model.save('cartpole_model.h5')  ## Save the Trained Model\n",
    "    del action_model\n",
    "    action_model = load_model('cartpole_model.h5')  ## Load the Trained Model\n",
    "\n",
    "## Here We test the trained model for 100 more games\n",
    "    Tests=[]  ## Initialize the Game Score List\n",
    "    for i in range(0,100):  ## Testing for 100 Games\n",
    "        observation = env.reset()\n",
    "        while(True):\n",
    "            old_observation = observation\n",
    "            q_values = get_q(action_model, observation)\n",
    "            action = np.argmax(q_values)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                Tests.append(iteration+1)\n",
    "                env.reset()\n",
    "                break\n",
    "    print(np.mean(Tests))\n",
    "    plt.plot(Tests)\n",
    "    plt.title('Testing Phase')\n",
    "    plt.ylabel('Time Steps')\n",
    "    plt.ylim(ymax=510)\n",
    "    plt.xlabel('Trial')\n",
    "    plt.savefig('Testing.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
